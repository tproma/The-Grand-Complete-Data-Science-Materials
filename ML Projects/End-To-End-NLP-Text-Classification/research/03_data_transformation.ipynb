{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\A_Category\\\\iNeuron\\\\End-To-End-NLP-Project-News-Article-Sorting'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\A_Category\\\\iNeuron\\\\End-To-End-NLP-Project-News-Article-Sorting'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen = True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    tokenizer_name: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ArticleSorting.constants import *\n",
    "from ArticleSorting.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self, \n",
    "                 config_filepath = CONFIG_FILE_PATH,\n",
    "                 params_filepath = PARAMS_FILE_PATH):\n",
    "        \n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_transformation_config(self) ->DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path= config.data_path,\n",
    "            tokenizer_name=config.tokenizer_name\n",
    "        )\n",
    "\n",
    "        return data_transformation_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ArticleSorting.logging import logger\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import Dataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name)\n",
    "        \n",
    "\n",
    "    def encode_categories(self):\n",
    "        df = pd.read_csv(self.config.data_path)\n",
    "        df['encoded_label'] = df['Category'].astype('category').cat.codes\n",
    "\n",
    "        '''Spliting the Data\n",
    "        # Training dataset\n",
    "        #train_data = df.sample(frac=0.8, random_state=42)\n",
    "        # Testing dataset\n",
    "        #test_data = df.drop(train_data.index)\n",
    "        '''\n",
    "\n",
    "        # Split train , test and validatio dataset\n",
    "        np.random.seed(112)\n",
    "        df_train, df_test, df_val = np.split(df.sample(frac=1, random_state=35),\n",
    "                                     [int(0.7*len(df)), int(0.8*len(df))])\n",
    "\n",
    "        print(len(df_train), len(df_test), len(df_val)) \n",
    "        \n",
    "\n",
    "        # Convert pyhton dataframe to Hugging Face arrow dataset\n",
    "        hg_train_data = Dataset.from_pandas(df_train)\n",
    "        hg_test_data = Dataset.from_pandas(df_test)\n",
    "        hg_val_data = Dataset.from_pandas(df_val)\n",
    "        print(hg_train_data, hg_test_data, hg_val_data)\n",
    "        return hg_train_data, hg_test_data, hg_val_data\n",
    "   \n",
    "\n",
    "\n",
    "    def tokenize_dataset(self, data):\n",
    "        return self.tokenizer(data[\"Text\"],\n",
    "                     max_length=512,\n",
    "                     truncation=True,\n",
    "                     padding=\"max_length\")\n",
    "    \n",
    "\n",
    "\n",
    "    def convert(self):\n",
    "        hg_train_data, hg_test_data, hg_val_data = self.encode_categories()\n",
    "\n",
    "        # Tokenize the dataset\n",
    "        dataset_train = hg_train_data.map(self.tokenize_dataset)\n",
    "        dataset_test = hg_test_data.map(self.tokenize_dataset)\n",
    "        dataset_val = hg_val_data.map(self.tokenize_dataset)\n",
    "\n",
    "        \n",
    "        # Remove the review and index columns because it will not be used in the model\n",
    "        dataset_train = dataset_train.remove_columns([\"ArticleId\", \"Text\", \"Category\", \"__index_level_0__\"])\n",
    "        dataset_test = dataset_test.remove_columns([\"ArticleId\", \"Text\", \"Category\", \"__index_level_0__\"])\n",
    "        dataset_val = dataset_val.remove_columns([\"ArticleId\", \"Text\", \"Category\", \"__index_level_0__\"])\n",
    "\n",
    "        # Rename label to labels because the model expects the name labels\n",
    "        dataset_train = dataset_train.rename_column(\"encoded_label\", \"labels\")\n",
    "        dataset_test = dataset_test.rename_column(\"encoded_label\", \"labels\")\n",
    "        dataset_val = dataset_val.rename_column(\"encoded_label\", \"labels\")\n",
    "\n",
    "\n",
    "        # Change the format to PyTorch tensors\n",
    "        dataset_train.set_format(\"torch\")\n",
    "        dataset_test.set_format(\"torch\")\n",
    "        dataset_val.set_format(\"torch\")\n",
    "\n",
    "        # Take a look at the data\n",
    "        print(dataset_train)\n",
    "        print(dataset_test)\n",
    "        print(dataset_val)\n",
    "\n",
    "        dataset_train.save_to_disk(os.path.join(self.config.root_dir,\"Train BBC dataset\"))\n",
    "        dataset_test.save_to_disk(os.path.join(self.config.root_dir,\"Test BBC dataset\"))\n",
    "        dataset_val.save_to_disk(os.path.join(self.config.root_dir,\"Validation BBC dataset\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-21 00:25:20,240:  INFO: common: yaml file:config\\config.yaml loaded successfully]\n",
      "[2023-11-21 00:25:20,245:  INFO: common: yaml file:params.yaml loaded successfully]\n",
      "[2023-11-21 00:25:20,247:  INFO: common: created directory at : artifacts]\n",
      "[2023-11-21 00:25:20,250:  INFO: common: created directory at : artifacts/data_transformation]\n",
      "1043 149 298\n",
      "Dataset({\n",
      "    features: ['ArticleId', 'Text', 'Category', 'encoded_label', '__index_level_0__'],\n",
      "    num_rows: 1043\n",
      "}) Dataset({\n",
      "    features: ['ArticleId', 'Text', 'Category', 'encoded_label', '__index_level_0__'],\n",
      "    num_rows: 149\n",
      "}) Dataset({\n",
      "    features: ['ArticleId', 'Text', 'Category', 'encoded_label', '__index_level_0__'],\n",
      "    num_rows: 298\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 1043\n",
      "})\n",
      "Dataset({\n",
      "    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 149\n",
      "})\n",
      "Dataset({\n",
      "    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 298\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    data_transformation.convert()\n",
    "  \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
